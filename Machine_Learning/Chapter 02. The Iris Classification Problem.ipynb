{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practical Machine Learning with Python\n",
    "# Chapter 2: One Machine Learning Project\n",
    "\n",
    "## Guillermo Avendaño-Franco  and Aldo Humberto Romero\n",
    "## West Virginia University\n",
    "\n",
    "### Machine Learning Workshop 2019\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks is based on a variety of sources, usually other notebooks, the material was adapted to the topics covered during lessons. In some cases, the original notebooks were created for Python 2.x or older versions of Scikit-learn or Tensorflow and they have to be adapted. \n",
    "\n",
    "## References\n",
    "\n",
    "### Books\n",
    "\n",
    " * **Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems**, 1st Edition *Aurélien Géron*  (2017)\n",
    "\n",
    " * **Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow**, 2nd Edition, *Sebastian Raschka* and *Vahid Mirjalili* (2017)\n",
    "\n",
    " * **Deep Learning: A Practitioner's approach**, *Josh Patterson* and *Adam Gibson* \n",
    " \n",
    " * **Deep Learning**, *Ian Goodfelow*, *Yoshua Bengio* and *Aaron Courville* (2016)\n",
    "\n",
    "### Jupyter Notebooks\n",
    "\n",
    " * [Yale Digital Humanities Lab](https://github.com/YaleDHLab/lab-workshops)\n",
    " \n",
    " * Aurelein Geron Hands-on Machine Learning with Scikit-learn \n",
    "   [First Edition](https://github.com/ageron/handson-ml)\n",
    "   [Second Edition (In preparation)](https://github.com/ageron/handson-ml2)\n",
    "   \n",
    " * [A progressive collection notebooks of the Machine Learning course by the University of Turin](https://github.com/rugantio/MachineLearningCourse)\n",
    "   \n",
    " * [A curated set of jupyter notebooks about many topics](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)\n",
    "   \n",
    "### Videos\n",
    "\n",
    " * [Caltech's \"Learning from Data\" by Professor Yaser Abu-Mostafa](https://work.caltech.edu/telecourse.html)\n",
    " \n",
    " \n",
    "The support of the National Science Foundation and the US Department of Energy under projects: DMREF-NSF 1434897, NSF OAC-1740111 and DOE DE-SC0016176 is recognized.\n",
    "\n",
    "<div style=\"clear: both; display: table;\">\n",
    "<div style=\"border: none; float: left; width: 40%; padding: 10px\">\n",
    "<img src=\"fig/NSF.jpg\" alt=\"National Science Foundation\" style=\"width:50%\" align=\"left\">\n",
    "    </div>\n",
    "    <div style=\"border: none; float: right; width: 40%; padding: 10px\">\n",
    "<img src=\"fig/DOE.jpg\" alt=\"National Science Foundation\" style=\"width:50%\" align=\"right\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "chapter_number = 2\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn: Introduction\n",
    "\n",
    "Scikit-learn is a collection of tools for machine learning written in Python:\n",
    "[http://scikit-learn.org](http://scikit-learn.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation of Data in Scikit-learn\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy.ndarray``, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can have a large dimension\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays.\n",
    "\n",
    "\n",
    "A classification algorithm, for example, expects the data to be represented as a **feature matrix** and a **label vector**:\n",
    "\n",
    "$$\n",
    "{\\rm feature~matrix:~~~} {\\bf X}~=~\\left[\n",
    "\\begin{matrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1D}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2D}\\\\\n",
    "x_{31} & x_{32} & \\cdots & x_{3D}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{N1} & x_{N2} & \\cdots & x_{ND}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm label~vector:~~~} {\\bf y}~=~ [y_1, y_2, y_3, \\cdots y_N]\n",
    "$$\n",
    "\n",
    "Here there are $N$ samples and $D$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Anderson's Iris Dataset\n",
    "\n",
    "As an example of a simple dataset, we're going to take a look at the iris data stored by scikit-learn.\n",
    "\n",
    "The Iris dataset is by far the earliest and the most commonly used dataset in the literature of pattern recognition. The dataset contains 150 instances of iris flowers collected in Hawaii. These instances are divided into 3 classes of Iris Setosa, Iris Versicolour and Iris Virginica:\n",
    "\n",
    " <table>\n",
    "  <tr>\n",
    "    <th><img src=\"fig/Iris-setosa-10_1.jpg\" alt=\"Iris Setosa\" style=\"width:200px\"></th>\n",
    "    <th><img src=\"fig/Iris-versicolor-21_1.jpg\" alt=\"Iris Versicolor\" style=\"width:200px\"></th>\n",
    "    <th><img src=\"fig/Iris-virginica-3_1.jpg\" alt=\"Iris Virginica\" style=\"width:200px\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris Setosa</td>\n",
    "    <td>Iris Versicolor</td>\n",
    "    <td>Iris Virginica</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "Each flower is distinguished based on 4 measures of sepal's width and length, and petal's width and length. These measures are taken for each iris flower:\n",
    "\n",
    "<img src=\"fig/iris.gif\" alt=\"Iris Setosa\" style=\"width:200px\">\n",
    "\n",
    "Detailed information of the dataset is listed next:\n",
    "\n",
    "    4 features with numerical values, with no missing data\n",
    "        sepal length in cm\n",
    "        sepal width in cm\n",
    "        petal length in cm\n",
    "        petal width in cm \n",
    "    3 classes, including Iris Setosa, Iris Versicolour, Iris Virginica\n",
    "    data size: 150 entries\n",
    "    data distribution: 50 entries for each class \n",
    "\n",
    "There are numerous technical papers that use Iris dataset. Here is a partial list:\n",
    "\n",
    " * **Fisher,R.A.** *The use of multiple measurements in taxonomic problems* Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\n",
    "\n",
    " * **Duda,R.O., & Hart,P.E.** *Pattern Classification and Scene Analysis* (1973) (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218.\n",
    "\n",
    " * **Dasarathy, B.V.** *Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments*. (1980)  IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
    "\n",
    " * **Gates, G.W.** *The Reduced Nearest Neighbor Rule*. (1972) IEEE Transactions on Information Theory, May 1972, 431-433. \n",
    "\n",
    "In the dataset, Iris Setosa is easier to be distinguished from the other two classes, while the other two classes are partially overlapped and harder to be separated.\n",
    "\n",
    "More information about this problem in Pattern and Image Recognicion:\n",
    " \n",
    " * On [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n",
    " \n",
    " * Descripton of [Iris Dataset](http://mirlab.org/jang/books/dcpr/dataSetIris.asp?title=2-2%20Iris%20Dataset) by Roger Jang as part of a online book on [Data Clustering and Pattern Recognition](http://mirlab.org/jang/books/dcpr/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Iris Data with Scikit-Learn\n",
    "\n",
    "Scikit-learn has a very straightforward set of data on these iris species.  The data consist of\n",
    "the following:\n",
    "\n",
    "- Features in the Iris dataset:\n",
    "\n",
    "  1. sepal length in cm\n",
    "  2. sepal width in cm\n",
    "  3. petal length in cm\n",
    "  4. petal width in cm\n",
    "\n",
    "- Target classes to predict:\n",
    "\n",
    "  1. Iris Setosa\n",
    "  2. Iris Versicolour\n",
    "  3. Iris Virginica\n",
    "  \n",
    "``scikit-learn`` embeds a copy of the iris CSV file along with a helper function to load it into numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset from scikit-learn is well organized into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = iris.data.shape\n",
    "print(\"Number of Samples:\", n_samples)\n",
    "print(\"Number of features:\", n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data and target are objects `numpy.ndarray` with the shapes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(iris.data))\n",
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example from the first 5 elements, each row is one entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target contains 150 digits corresponding to the 3 classes of Iris flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is four dimensional, but we can visualize two of the dimensions\n",
    "at a time using a simple scatter-plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_index = 2\n",
    "y_index = 3\n",
    "\n",
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index],\n",
    "            c=iris.target)\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the relations can be shown as a grid of 4x4 subplots in matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "fig, axes=plt.subplots(nrows=4, ncols=4, figsize=(16,16))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[i,j].scatter(iris.data[:, j], iris.data[:, i], c=iris.target)\n",
    "        if i==3: axes[i,j].set_xlabel(iris.feature_names[j])\n",
    "        if j==0: axes[i,j].set_ylabel(iris.feature_names[i]);\n",
    "        \n",
    "#plt.colorbar(ticks=[0, 1, 2], format=formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure is clear that **setosa** variety is fairly separated from the **versicolor** and **virginica** and those two in turn are difficult to separate but for some features the distinction is clear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression as Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical method which is used to analyze datasets in which there are one or more independent variables that determine a binary outcome (True/False, 1/0). The goal of this methodology is to find the best fitting model to describe the relation between the independent input variables and the dichotomous outcome. The probability distribution would have the form of an s-shape as:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{p}{1-p} \\right) = a + b_1 x_1 + b_2 x_2 \\cdots\n",
    "$$\n",
    "\n",
    "where p is the probability of presence of the characteristic of interest, $x_1, x_2, \\cdots$ are the independent input variables and $a, b_1, b_2, \\cdots$ are the model fitted parameters. As here we want to optimize the probability distribution, we do not use the square error minimization but to maximize the likelihood of observing the sample values. This statistical process can be found in many different machine learning books or in [Logistic Regression lecture](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int32)  # 1 if Iris-Virginica, else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: To be future-proof we set `solver=\"lbfgs\"` since this will be the default value in Scikit-Learn 0.22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure in the book actually is actually a bit fancier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")\n",
    "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "plt.arrow(decision_boundary[0], 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "plt.arrow(decision_boundary[0], 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 3, -0.02, 1.02]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int32)\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=10**10, random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
    "\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "\n",
    "\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.5, \"Not Iris-Virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris-Virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic but 3 classes, where the largest is defined by the statistical analysis of occurrence\n",
    "# of the training data\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg.predict_proba([[5, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Available Data\n",
    "\n",
    "Scikit learn offers a collection of small datasets like iris for learning purposes. In adition to that, scikit also include functions to download and prepare largers datasets. Finally some datasets can be generated randomly under some models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small datasets\n",
    "\n",
    "These **small** datasets are packaged with the scikit-learn installation, and can be downloaded using the tools in ``sklearn.datasets.load_*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in sklearn.datasets.__dict__.keys() if x[:5]=='load_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset fetcher\n",
    "\n",
    "These larger datasets are available for download (BECAREFUL WITH THE CONNECTION!), and scikit-learn includes tools which streamline this process.  These tools can be found in\n",
    "  ``sklearn.datasets.fetch_*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in sklearn.datasets.__dict__.keys() if x[:6]=='fetch_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeled datasets\n",
    "\n",
    "Finally, there are several datasets which are generated from models based on a random seed.  These are available in the ``sklearn.datasets.make_*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in sklearn.datasets.__dict__.keys() if x[:5]=='make_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Scikit-learn Estimator Object\n",
    "\n",
    "Every algorithm is exposed in scikit-learn via an ''Estimator'' object (initialization of the model). That means that you first prepare the object with some parameters and later you apply the `fit` method (in most cases) to process the data. After that predictions can be made. Basically, the process in SciKit is always the same: import the model, initialize the model, train or fit the model and use the model to predict. \n",
    "\n",
    "For instance, consider a linear regression as it is implemented on the linear model of scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LinearRegression(normalize=True)\n",
    "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimated Model parameters**: When data is *fit* with an estimator, parameters are estimated from the data at hand. All the estimated parameters are attributes of the estimator object ending by an underscore:\n",
    "\n",
    "For example consider as five points in the $x$ domain and the function  $y=f(x)$ will include some small randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(5) \n",
    "y = 0.9*np.arange(5)+ 0.5*(np.random.rand(5)-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot those points with a quick plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.xlim(-0.5, 4.5)\n",
    "plt.ylim(-0.5, 4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn need the input data as 2D array instead of a unidimensional array. The solution is to add a new axis to the original x array using `numpy.newaxis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data for sklearn is 2D: (samples == 3 x features == 1)\n",
    "X = x[:, np.newaxis]\n",
    "print (X)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y) \n",
    "print (model[1].coef_)\n",
    "print (model[1].intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model found a line with a slope $\\approx 0.84$ and intercept $\\approx 0.18$, slightly different from the slopw $0.9$ and itercept $0.0$ if random numbers will not created deviations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxonomy of Machine Learning Algorithms\n",
    "\n",
    "There are several ways of classify the wide field of Machine Learning. Algorithms can be classified by the dimensionality of the input and output, if they deal with discrete (categorical) input or output and the basic algorithm underlying the solution. However a classical classification is based on existence or absents of known output in the problem proposed.\n",
    "\n",
    "## Supervised,  Unsupervised and Reinforcement learning\n",
    "\n",
    "The algorithms of machine learning are generally split into two basic categories: **Supervised**, **Unsupervised** and reinforcement learning.\n",
    "\n",
    "### Supervised Learning\n",
    "**Supervised** learning concerns **labeled** data, and the construction of models that can be used to predict labels for new, unlabeled data.\n",
    "\n",
    "You can think about these problems as having a set of two sets: \n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\\; \\mathbf{input\\ data} \\; + \\; \\mathbf{correct\\ result} \\; \\right] \\rightarrow \\mathbf{predict\\ results\\ for\\ new\\ data}\n",
    "\\end{equation}\n",
    "\n",
    "*Example:* Given a set of *labeled* hand-written digits, create an algorithm which will predict the label of a new instance for which a label is not known.\n",
    "\n",
    "### Unsupervised Learning\n",
    "**Unsupervised** learning concerns **unlabeled** data, and finding structure in the data such as clusters, important dimensions, etc.\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\\; \\mathbf{input\\ data} \\; + \\; ? \\; \\right] \\rightarrow \\mathbf{discover\\ structure\\ present\\ in\\ the\\ data}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*Example:* Given a set of *unlabeled* digits, determine which digits are related.\n",
    "\n",
    "### Reinforcement Learning\n",
    "**Reinforcement** learning concerns data with poor knowledge on what the correct results looks like, but we are able to provide a function that grades how good a particular solution should be.\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\\; \\mathbf{input\\ data} \\; + \\; \\mathit{some\\ output} \\; + \\mathbf{grade\\ function} \\;\\right] \\rightarrow \\mathbf{the\\ better\\ strategy\\ to\\ achieve\\ a\\ solution}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*Example:* The algorithms used in autonomous vehicles or in learning to play a game against a human opponent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Classification and Regression\n",
    "\n",
    "In **Supervised Learning**, we have a dataset consisting of both features and labels.\n",
    "The task is to construct an estimator which is able to predict the label of an object\n",
    "given the set of features. A relatively simple example is predicting the species of \n",
    "iris given a set of measurements of its flower. This is a relatively simple task. \n",
    "Some more complicated examples are:\n",
    "\n",
    "- given a multicolor image of an object through a telescope, determine\n",
    "  whether that object is a star, a quasar, or a galaxy.\n",
    "- given a photograph of a person, identify the person in the photo.\n",
    "- given a list of movies a person has watched and their personal rating\n",
    "  of the movie, recommend a list of movies they would like\n",
    "  (So-called *recommender systems*: a famous example is the [Netflix Prize](http://en.wikipedia.org/wiki/Netflix_prize)).\n",
    "\n",
    "What these tasks have in common is that there is one or more unknown\n",
    "quantities associated with the object which needs to be determined from other\n",
    "observed quantities.\n",
    "\n",
    "Supervised learning is further broken down into two categories, **classification** and **regression**.\n",
    "In classification, the label is discrete, while in regression, the label is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with K nearest neighbors\n",
    "K nearest neighbors (kNN) is one of the simplest non parametric learning strategies that can be used for classification and regression.\n",
    "kNN is one of the simpler algorithm that stores all available cases and predict the numerical target based on a similarity measure (e.g., distance functions which are based on a metric definition, where the Euclidean, Manhattan or Minkowski are the most used ones). An important detail to keep in mine is that it is desirable that all features should be measured on the same scale. In case the scale is not the same, it is advisable that the scale is standarized.\n",
    "\n",
    "The algorithm can be summarized as follows: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class.\n",
    "\n",
    "Let's try it out on our iris classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# create the model\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit the model\n",
    "knn.fit(X, y)\n",
    "\n",
    "# What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?\n",
    "# call the \"predict\" method:\n",
    "result = knn.predict([[3, 5, 4, 2],])\n",
    "\n",
    "print (iris.target_names[result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this kNN we can create a map of all the different outcomes fixing the values of 3cm x 5cm sepal. Here we will use the linspace function from numpy that return an evenly spaced numbers over a specified interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N=100\n",
    "preds = np.zeros((N,N))\n",
    "x0 = 3\n",
    "x1 = 5\n",
    "minx2=np.min(iris.data[:,2])\n",
    "maxx2=np.max(iris.data[:,2])\n",
    "x2 = np.linspace(minx2, maxx2, N, endpoint=True)\n",
    "minx3=np.min(iris.data[:,3])\n",
    "maxx3=np.max(iris.data[:,3])\n",
    "x3 = np.linspace(minx3, maxx3, N, endpoint=True)\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        preds[i,j]=knn.predict([[x0, x1 , x2[i], x3[j]],])\n",
    "\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "        \n",
    "plt.imshow(preds[::-1], extent=[minx2,maxx2, minx3, maxx3],aspect='auto')\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the support vector machine (SVM) algorithm is to find a hyperplane in an N-dimensional space(where N is the number of features) that distinctly classifies the data points.\n",
    "This algorithm receive a given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side of the plane.\n",
    "\n",
    "To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(gamma='scale')\n",
    "model.fit(X, y)\n",
    "result = model.predict([[3, 5, 4, 2],])\n",
    "print (iris.target_names[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=(5,)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=100\n",
    "preds = np.zeros((N,N))\n",
    "x0 = 3\n",
    "x1 = 5\n",
    "minx2=np.min(iris.data[:,2])\n",
    "maxx2=np.max(iris.data[:,2])\n",
    "x2 = np.linspace(minx2, maxx2, N, endpoint=True)\n",
    "minx3=np.min(iris.data[:,3])\n",
    "maxx3=np.max(iris.data[:,3])\n",
    "x3 = np.linspace(minx3, maxx3, N, endpoint=True)\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        preds[i,j]=model.predict([[x0, x1 , x2[i], x3[j]],])\n",
    "\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "        \n",
    "plt.imshow(preds[::-1], extent=[minx2,maxx2,minx3,maxx3],aspect='auto')\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Example\n",
    "\n",
    "Simplest possible regression is fitting a line to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some simple data\n",
    "np.random.seed(0)\n",
    "X = np.random.random(size=(20, 1))\n",
    "y = 3 * X.squeeze() + 2 + np.random.normal(size=20)\n",
    "\n",
    "# Fit a linear regression to it\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)\n",
    "print (\"Model coefficient: %.5f, and intercept: %.5f\"\n",
    "       % (model.coef_, model.intercept_))\n",
    "\n",
    "# Plot the data and the model prediction\n",
    "X_test = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "plt.plot(X_test.squeeze(), y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning: Dimensionality Reduction and Clustering\n",
    "\n",
    "**Unsupervised Learning** addresses a different sort of problem. Here the data has no labels,\n",
    "and we are interested in finding pattern or structure between the objects in question. In a sense,\n",
    "you can think of unsupervised learning as a means of discovering labels from the data itself.\n",
    "Unsupervised learning comprises tasks such as *dimensionality reduction*, *clustering*, and\n",
    "*density estimation*. For example, in the iris data discussed above, we can used unsupervised\n",
    "methods to determine combinations of the measurements which best display the structure of the\n",
    "data. As we'll see below, such a projection of the data can be used to visualize the\n",
    "four-dimensional dataset in two dimensions. Some more involved unsupervised learning problems are:\n",
    "\n",
    "- given detailed observations of distant galaxies, determine which features or combinations of\n",
    "  features best summarize the information.\n",
    "- given a mixture of two sound sources (for example, a person talking over some music),\n",
    "  separate the two (this is called the [blind source separation](http://en.wikipedia.org/wiki/Blind_signal_separation) problem).\n",
    "- given a video, isolate a moving object and categorize in relation to other moving objects which have been seen.\n",
    "- A given crystal structure, we can use to predict the stability.\n",
    "\n",
    "Sometimes the two may even be combined: e.g. Unsupervised learning can be used to find useful\n",
    "features in heterogeneous data, and then these features can be used within a supervised\n",
    "framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction: PCA\n",
    "\n",
    "Principle Component Analysis (PCA) is a dimension reduction technique that can find the combinations of variables that explain the most variance. This method is one of the most popular linear dimension reduction methodologies available in machine learning. Sometimes, it is used alone and sometimes as a starting solution for other dimension reduction methods. PCA is a projection based method which transforms the data by projecting it onto a set of orthogonal axes.\n",
    "\n",
    "Consider the iris dataset. It cannot be visualized in a single 2D plot, as it has 4 features. We are going to extract 2 combinations of sepal and petal dimensions to visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "print (\"Reduced dataset shape:\", X_reduced.shape)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y)\n",
    "\n",
    "print (\"Meaning of the 2 components:\")\n",
    "for component in pca.components_:\n",
    "    print (\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering: K-means\n",
    "\n",
    "K-means clustering is one of the simplest and most popular unsupervised machine learning algorithms by practicioners.\n",
    "As discussed before, unsupervised algorithms make inferences from datasets using only input vectors without referring to known, or labelled, outcomes.\n",
    "AndreyBu, who has more than 5 years of machine learning experience and currently teaches people his skills, says that “the objective of K-means is simple: group similar data points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.”\n",
    "\n",
    "The main idea with this algorithm is to group similar data into clusters and try to find the underlying developed pattern by looking how the data clusters into a fixed number of (k) clusters in the dataset.\n",
    "\n",
    "Note that these clusters will uncover relevent hidden structure of the data only if the criterion used highlights it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=3, random_state=0) # Fixing the RNG in kmeans\n",
    "k_means.fit(X)\n",
    "y_pred = k_means.predict(X)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn's estimator interface\n",
    "\n",
    "Scikit-learn strives to have a uniform interface across all methods. Given a scikit-learn *estimator*\n",
    "object named `model`, the following methods are available:\n",
    "\n",
    "- Available in **all Estimators**\n",
    "  + `model.fit()` : fit training data. For supervised learning applications,\n",
    "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
    "    For unsupervised learning applications, this accepts only a single argument,\n",
    "    the data `X` (e.g. `model.fit(X)`).\n",
    "- Available in **supervised estimators**\n",
    "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
    "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
    "    and returns the learned label for each object in the array.\n",
    "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
    "    this method, which returns the probability that a new observation has each categorical label.\n",
    "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
    "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "- Available in **unsupervised estimators**\n",
    "  + `model.transform()` : given an unsupervised model, transform new data into the new basis.\n",
    "    This also accepts one argument `X_new`, and returns the new representation of the data based\n",
    "    on the unsupervised model.\n",
    "  + `model.fit_transform()` : some estimators implement this method,\n",
    "    which more efficiently performs a fit and a transform on the same input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Scikit-Learn: Validation and Model Selection\n",
    "\n",
    "This section focuses on **validation** and **model selection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Example: Classifying Digits\n",
    "\n",
    "Features can be any **uniformly measured** numerical observation of the data. For example, in the digits data, the features are the brightness of each pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Gaussian Naive Bayes Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show a quick classification example, using the simple-and-fast Gaussian Naive Bayes estimator. Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. Basically, Bayes’ Theorem provides a mathematical procedure where we can calculate the probability of a hypothesis given our prior knowledge. This can be described by this equation\n",
    "\n",
    "$$\n",
    "P[h|d] = \\frac{P[d|h] P[h]}{P[d]}\n",
    "$$\n",
    "where $P$ is a probability distribution function, $d$ is the historic data and $h$ is the tested hypothesis. Therefore, this equation can be read as follows: the conditional probability (posterior) of a given hypothesis occurs provided  the data $d$ can be obtained as the fraction between the product of the probability that the data $d$ occurs given that $h$ is true with the probability of the hypothesis divided by the probability of the data.\n",
    "\n",
    "Now, the posterior probability can be obtained from a number of different hypotheses. Based on this, the maximum probable hypothesis (MAP: maximum probably hypothesis) can be calculated and  \n",
    "After calculating the posterior probability for a number of different hypotheses, you can select the hypothesis with the highest probability. This is the maximum probable hypothesis and may formally be called the maximum a posteriori (MAP) hypothesis. How we account on the different probability terms give rise to different methods.\n",
    "\n",
    "In the so called naive Bayes, the probabilities for each hypothesis are simplified, such that the calculation becomes easier. This approximation will say that the data realization to obtain $P[d|h]$ is independent, which means that\n",
    "$P[d|h] =  P[d_1|h] P[d_2|h] P[d_3|h] \\cdots$.\n",
    "\n",
    "Now, the method reduces to the following. By entering a set of classified data (training), we can use it to obtain the probabilities of each class and the conditional probabilities of each input value given each class. This last probability is obtained directly from the training data as the frequency of each feature in a given class divided by the frequency of instaces of that the class value. \n",
    "\n",
    "A very simple case is observed as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Instantiate the estimator\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Fit the estimator to the data, leaving out the last five samples\n",
    "clf.fit(X[:-5], y[:-5])\n",
    "\n",
    "# Use the model to predict the last several labels\n",
    "y_pred = clf.predict(X[-5:])\n",
    "\n",
    "print (y_pred)\n",
    "print (y[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this relatively simple model leads to a perfect classification of the last few digits!\n",
    "\n",
    "Let's use the model to predict labes for the full dataset, and plot the **confusion matrix**, which is a convenient visual representation of how well the classifier performs.\n",
    "\n",
    "By definition a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in $i$ but predicted to be in group $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred),\n",
    "               cmap=plt.cm.binary, interpolation='none')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('true value')\n",
    "    plt.ylabel('predicted value')\n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y, y_pred))\n",
    "plot_confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, there is confusion between some values.  In particular, the number **2** is often mistaken for the number **8** by this model!  But for the vast majority of digits, we can see that the classification looks correct.\n",
    "\n",
    "Let's use the ``metrics`` submodule again to print the accuracy of the classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an 82% accuracy rate with this particular model.\n",
    "\n",
    "But there's a problem: we are testing the model on the data we used to train the model. As we'll see later, this is generally not a good approach to model validation!  Because of the nature of the Naive Bayes estimator, it's alright in this case, but we'll see later examples where this approach causes problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Prediction Performance\n",
    "\n",
    "An important piece of the learning task is the measurement of prediction performance, also known as **model validation**.  We'll go into detail about this, but first motivate the approach with an example.\n",
    "\n",
    "## The Importance of Splitting\n",
    "Above we looked at a *confusion matrix*, which can be computed based on the results of any model. Let's look at another classification scheme here, the *K-Neighbors Classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y, y_pred))\n",
    "plot_confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier gives perfect results!  Have we settled on a perfect classification scheme?\n",
    "\n",
    "**No!**  The *K*-neighbors classifier is an example of an instance-based classifier, which memorizes the input data and compares any unknown sample to it.  To accurately measure the performance, we need to use a separate *validation set*, which the model has not yet seen.\n",
    "\n",
    "Scikit-learn contains utilities to split data into a training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a more accurate indication of how well the model is performing.\n",
    "\n",
    "For this reason you should **always do a train/test split** when validating a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Validation Metrics\n",
    "\n",
    "Above, we used perhaps the most simple evaluation metric, the number of matches and mis-matches.  But this is not always sufficient.  For example, imagine you have a situation where you'd like to identify a rare class of event from within a large number of background sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an un-balanced 2D dataset\n",
    "np.random.seed(0)\n",
    "X = np.vstack([np.random.normal(0, 1, (950, 2)),\n",
    "               np.random.normal(-1.8, 0.8, (50, 2))])\n",
    "y = np.hstack([np.zeros(950), np.ones(50)])\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='none',\n",
    "            cmap=plt.cm.Accent);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring other Validation Scores\n",
    "\n",
    "Until now we are using only the **accuracy** to evaluate our algorithms. We can calculate other scores such as the **precision**, the **recall**, and the **f1 score**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "clf = SVC(kernel='linear').fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print (\"accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print (\"precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print (\"recall:\", metrics.recall_score(y_test, y_pred))\n",
    "print (\"f1 score:\", metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do these mean?\n",
    "\n",
    "These are ways of taking into account not just the classification results, but the results **relative to the true category**.\n",
    "\n",
    " $$ {\\rm accuracy} \\equiv \\frac{\\rm correct~labels}{\\rm total~samples} $$\n",
    "\n",
    "---\n",
    "\n",
    " $$ {\\rm precision} \\equiv \\frac{\\rm true~positives}{\\rm true~positives + false~positives} $$\n",
    "\n",
    "---\n",
    "\n",
    " $$ {\\rm recall} \\equiv \\frac{\\rm true~positives}{\\rm true~positives + false~negatives} $$\n",
    "\n",
    "---\n",
    "\n",
    " $$ F_1 \\equiv 2 \\frac{\\rm precision \\cdot recall}{\\rm precision + recall} $$\n",
    "\n",
    "The **accuracy**, **precision**, **recall**, and **f1-score** all range from 0 to 1, with 1 being optimal.\n",
    "Here we've used the following definitions:\n",
    "\n",
    "- *True Positives* are those which are labeled ``1`` which are actually ``1``\n",
    "- *False Positives* are those which are labeled ``1`` which are actually ``0``\n",
    "- *True Negatives* are those which are labeled ``0`` which are actually ``0``\n",
    "- *False Negatives* are those which are labeled ``0`` which are actually ``1``\n",
    "\n",
    "\n",
    "We can quickly compute a summary of these statistics using scikit-learn's provided convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(y_test, y_pred,\n",
    "                                    target_names=['background', 'foreground']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that, though the overall correct classification rate is 97%, we only correctly identify 67% of the desired samples, and those that we label as positives are only 83% correct!  This is why you should make sure to carefully choose your metric when validating a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "Using the simple train/test split as above can be useful, but there is a disadvantage: **You're ignoring a portion of your dataset**.  One way to address this is to use cross-validation.\n",
    "\n",
    "The simplest cross-validation scheme involves running two trials, where you split the data into two parts, first training on one, then training on the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5)\n",
    "print (X1.shape)\n",
    "print (X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = SVC(kernel='linear').fit(X1, y1).predict(X2)\n",
    "y1_pred = SVC(kernel='linear').fit(X2, y2).predict(X1)\n",
    "\n",
    "print (np.mean([metrics.precision_score(y1, y1_pred),\n",
    "               metrics.precision_score(y2, y2_pred)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as **two-fold** cross-validation, and is a special case of *K*-fold cross validation.\n",
    "\n",
    "Because it's such a common routine, scikit-learn has a K-fold cross-validation scheme built-in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Let's do a 2-fold cross-validation of the SVC estimator\n",
    "print (cross_val_score(SVC(kernel='linear'), X, y, cv=2, scoring='precision'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to use ``sklearn.cross_validation.KFold`` and ``sklearn.cross_validation.StratifiedKFold`` directly, as well as other cross-validation models which you can find in the ``cross_validation`` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: The ``SVC`` classifier takes a parameter ``C`` whose default value is ``1``.  Using 5-fold cross-validation, make a plot of the precision as a function of ``C``, for the ``SVC`` estimator on this dataset.  For best results, use a logarithmic spacing of ``C`` between 0.1 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cs = np.logspace(-1.5, 2, 10)\n",
    "scores = []\n",
    "\n",
    "for C in Cs:\n",
    "    score = cross_val_score(SVC(kernel='linear', C=C), X, y, cv=5, scoring='precision')\n",
    "    scores.append(score.mean())\n",
    "\n",
    "plt.semilogx(Cs, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "The previous exercise is an example of a **grid search** for model evaluation.  Again, because this is such a common task, Scikit-learn has a grid search tool built-in, which is used as follows.  Note that ``GridSearchCV`` has a ``fit`` method: it is a meta-estimator: an estimator over estimators!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "Crange = np.logspace(-1.5, 2, 10)\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid={'C': Crange},\n",
    "                    scoring='precision', cv=5)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print (\"best parameter choice:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [g for g in grid.cv_results_['mean_test_score']]\n",
    "plt.semilogx(Crange, scores);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search can come in very handy when you're tuning a model for a particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = chapter_number\n",
    "maxt=(2*(n-1)+3)*np.pi/2\n",
    "t = np.linspace(np.pi/2, maxt, 1000)\n",
    "tt= 1.0/(t+0.01)\n",
    "x = (maxt-t)*np.cos(t)**3\n",
    "y = t*np.sqrt(np.abs(np.cos(t))) + np.sin(0.3*t)*np.cos(2*t)\n",
    "plt.plot(x, y, c=\"green\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f'Chapter {chapter_number} run in {int(end - start):d} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
