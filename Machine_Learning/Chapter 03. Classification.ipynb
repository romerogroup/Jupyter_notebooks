{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practical Machine Learning with Python\n",
    "# Chapter 3: Classification\n",
    "## Guillermo Avendaño-Franco  and Aldo Humberto Romero\n",
    "## West Virginia University\n",
    "\n",
    "### Machine Learning Workshop 2019\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks is based on a variety of sources, usually other notebooks, the material was adapted to the topics covered during lessons. In some cases, the original notebooks were created for Python 2.x or older versions of Scikit-learn or Tensorflow and they have to be adapted. \n",
    "\n",
    "## References\n",
    "\n",
    "### Books\n",
    "\n",
    " * **Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems**, 1st Edition *Aurélien Géron*  (2017)\n",
    "\n",
    " * **Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow**, 2nd Edition, *Sebastian Raschka* and *Vahid Mirjalili* (2017)\n",
    "\n",
    " * **Deep Learning: A Practitioner's approach**, *Josh Patterson* and *Adam Gibson* \n",
    " \n",
    " * **Deep Learning**, *Ian Goodfelow*, *Yoshua Bengio* and *Aaron Courville* (2016)\n",
    "\n",
    "### Jupyter Notebooks\n",
    "\n",
    " * [Yale Digital Humanities Lab](https://github.com/YaleDHLab/lab-workshops)\n",
    " \n",
    " * Aurelein Geron Hands-on Machine Learning with Scikit-learn \n",
    "   [First Edition](https://github.com/ageron/handson-ml)\n",
    "   [Second Edition (In preparation)](https://github.com/ageron/handson-ml2)\n",
    "   \n",
    " * [A progressive collection notebooks of the Machine Learning course by the University of Turin](https://github.com/rugantio/MachineLearningCourse)\n",
    "   \n",
    " * [A curated set of jupyter notebooks about many topics](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)\n",
    "   \n",
    "### Videos\n",
    "\n",
    " * [Caltech's \"Learning from Data\" by Professor Yaser Abu-Mostafa](https://work.caltech.edu/telecourse.html)\n",
    " \n",
    " \n",
    "The support of the National Science Foundation and the US Department of Energy under projects: DMREF-NSF 1434897, NSF OAC-1740111 and DOE DE-SC0016176 is recognized.\n",
    "\n",
    "<div style=\"clear: both; display: table;\">\n",
    "<div style=\"border: none; float: left; width: 40%; padding: 10px\">\n",
    "<img src=\"fig/NSF.jpg\" alt=\"National Science Foundation\" style=\"width:50%\" align=\"left\">\n",
    "    </div>\n",
    "    <div style=\"border: none; float: right; width: 40%; padding: 10px\">\n",
    "<img src=\"fig/DOE.jpg\" alt=\"National Science Foundation\" style=\"width:50%\" align=\"right\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "chapter_number = 2\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where we are in the map?\n",
    "\n",
    "<img src=\"fig/ml_map.png\" width=\"700\" height=\"200\" title=\"Precision vs Recall\" alt=\"Precision vs Recall\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Regression are the foundations of many of the algorithms for Machine Learning, particular supervised learning. In this case the problem is to identify to which class or category new data belongs once we have trained the model with a set of data for which we know the output labels.\n",
    "\n",
    "In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.\n",
    "\n",
    "The following exercise provide a nice introduction to many of the relevant concepts in classification, some of them still valid when move into the prediction of continuum values in regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST database\n",
    "\n",
    "The MNIST database of handwritten digits, available from the link below, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting\n",
    "\n",
    "Many methods have been tested with this training set and test set. For more information about the original datasets from NIST see [Yann LeCun Website](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, data_home=\".\", cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.to_numpy()\n",
    "y=y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a vector with 784 components corresponding to a matrix 28 x 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Rows\n",
    "rgrid=4\n",
    "# Number of Columns\n",
    "cgrid=10\n",
    "fig, axes = plt.subplots(rgrid,cgrid, figsize=(16,6))\n",
    "for i in range(rgrid):\n",
    "    for j in range(cgrid):\n",
    "        #print(i,j,cgrid*i+j)\n",
    "        axes[i,j].imshow(X[cgrid*i+j].reshape(28,28),interpolation='none',cmap='gray')\n",
    "        axes[i,j].xaxis.set_ticks_position('none')\n",
    "        axes[i,j].yaxis.set_ticks_position('none')\n",
    "        axes[i,j].set_yticklabels([])\n",
    "        axes[i,j].set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:12].reshape(2,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, **options)\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "plot_digits(example_images, images_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifier\n",
    "\n",
    "Later we will attack the problem of classify all the ten digits. For now lets focus on a more modest classification problem, where we will try to identify if a given number is 5 or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: some hyperparameters will have a different defaut value in future versions of Scikit-Learn, such as `max_iter` and `tol`. To be future-proof, we explicitly set these hyperparameters to their future default values. For simplicity, this is not shown in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several classifiers implemented in Scikit-Learn, the Stochastic gradient descent (SGD) `SGDClassifier` is a simple yet very efficient approach to fit linear models. While SGD is in reality an iterative method used to optimize the solution in an objective function with smooth properties but doing that stochastically, the classifier with SGD uses this method but now with a regularized linear model. \n",
    "It is particularly useful when the number of samples (and the number of features) is very large as it does not need to have the whole database in RAM. This process is then obtained by following the negative gradient of the objective after seeing only a single or a few training data. \n",
    "\n",
    "A nice discussion on the Stochastic gradient descent can be found at [SGD site](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/), where we have taken the figures below. Left hand side represents a batch gradient descent where a batch of random data is used and right hand side is the stochastic gradient descent where a single random sample has been used for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='fig/gdp.png' alt=\"Batch Gradient Descent\" style=\"width:60%\"></td><td><img src='fig/gdp-1.jpg' alt=\"Stochastic Gradient Descent\" style=\"width:60%\"></td></tr></table>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the point is to get a black box that do classification for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.predict(X_train[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that indeed the classifier seems to work fine. How to quantify that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "The first measure will be accuracy, the total number of right answers versus the total number of misses\n",
    "\n",
    "\\begin{equation}\n",
    "accuracy = \\frac{\\text{Total number of right answers}}{\\text{Total number of cases}}\n",
    "\\end{equation}\n",
    "\n",
    "We can do that for several subsets of the train set, in what is called **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can create a new classifier that always answers negatively no matter what vector is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 90% accuracy, which is consistent with a set of many cases where each digit is represented with the same frequency than any other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "A better way of looking at the performance of a classifier is via the confusion matrix. A matrix that for each instace can be divided into 4 sets\n",
    "\n",
    " * **True positives** Images that are 5s and were identified as 5s\n",
    " * **True negatives** Images that are NOT 5s and were identified as NOT 5s\n",
    " * **False positives** Images that are NOT 5s but were identified as 5s\n",
    " * **False negatives** Images that are 5s but were identified as NOT 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "y_train_pred = sgd_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect confusion matrix will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_predictions = y_train_5  # pretend we reached perfection\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1\n",
    "\n",
    "<img src=\"fig/Precision_Recall.png\" width=\"700\" height=\"200\" title=\"Precision vs Recall\" alt=\"Precision vs Recall\"></a>\n",
    "\n",
    "Instead of using the **accuracy** a better metric could be use the precision and recall as defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "precision = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "\\end{equation}\n",
    "\n",
    "---\n",
    "\n",
    "\\begin{equation}\n",
    "recall = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "\\end{equation}\n",
    "\n",
    "Finally there is one metric that measures the balance between precision and recall\n",
    "\n",
    "\\begin{equation}\n",
    "F_1 = \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP = 4820\n",
    "# FP = 2263\n",
    "4820 / (4820 + 2263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP = 4820\n",
    "# FN = 601\n",
    "4820 / (4820 + 601)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4820 / (4820 + (601 + 2263) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Function\n",
    "\n",
    "Scikit-learn provides a function to show the value that the SGD assign to each set of inputs. See for example the first 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_scores = sgd_clf.decision_function(X_train[:12])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that images indentified with 5 received positive score and other numbers received negative scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the threshold \n",
    "\n",
    "one interesting observation is to notice that if by any means we could change the threshold of acceptance for considering a image a 5 we increase the precision (by reducing FP) but at expenses of also reducing the recall (by also increasing the FN). In summary precision and recall are two conflicting quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 8000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "    plt.grid(True)                              # Not shown\n",
    "    plt.axis([-50000, 50000, 0, 1])             # Not shown\n",
    "\n",
    "plt.figure(figsize=(8, 4))                      # Not shown\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.plot([7813, 7813], [0., 0.9], \"r:\")         # Not shown\n",
    "plt.plot([-50000, 7813], [0.9, 0.9], \"r:\")      # Not shown\n",
    "plt.plot([-50000, 7813], [0.4368, 0.4368], \"r:\")# Not shown\n",
    "plt.plot([7813], [0.9], \"ro\")                   # Not shown\n",
    "plt.plot([7813], [0.4368], \"ro\")                # Not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train_pred == (y_scores > 0)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.plot([0.4368, 0.4368], [0., 0.9], \"r:\")\n",
    "plt.plot([0.0, 0.4368], [0.9, 0.9], \"r:\")\n",
    "plt.plot([0.4368], [0.9], \"ro\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores >= threshold_90_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curves\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) is a plot of recall (also known as *True Positive Rate*) against False Positive Rate (FPR) that is one minus the True Negative Rate (TNR) (Negative instances correctly indentified as negative)\n",
    "\n",
    " * FPR = 1 - TNR = 1 - specificity\n",
    " * TPR = Recall = sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n",
    "    plt.grid(True)                                            # Not shown\n",
    "\n",
    "plt.figure(figsize=(8, 6))                         # Not shown\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\") # Not shown\n",
    "plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")  # Not shown\n",
    "plt.plot([4.837e-3], [0.4368], \"ro\")               # Not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we set `n_estimators=100` to be future-proof since this will be the default value in Scikit-Learn 0.22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
    "                                    method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\")\n",
    "plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")\n",
    "plt.plot([4.837e-3], [0.4368], \"ro\")\n",
    "plt.plot([4.837e-3, 4.837e-3], [0., 0.9487], \"r:\")\n",
    "plt.plot([4.837e-3], [0.9487], \"ro\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\n",
    "precision_score(y_train_5, y_train_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case until now was a binary classifier, but the MNIST is actually a multiclass classification problem.\n",
    "Therefore, the goal is to construct a function which, given a new data point, will correctly predict the class to which the new data belongs. \n",
    "\n",
    "There are several Multiclass classifiers in Scikit-learn. Some are binary, others are directly multiclass as for example KNN, decision trees, neural networks, logistic regression, Naive Bayes, \n",
    "\n",
    "The simplest would be the OVA method (one vs all), which basically treat the multiclass classification as a set of binary classification. Here we train each binary classifier independently to solve the binary task. A simple example is seen in the left hand side of figure below. Each figure corresponds to a single class.   \n",
    "\n",
    "Note: depending on the binary classifier is set up, there is a change that you get contradictory results, specially if you have imbalance data. If that happens, it is a good idea to use a discriminant function or using the loss function. A nice intro about this idea can be found in [Loss function description](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23).\n",
    "\n",
    "The other possibility is to use one-to-one classification, which basically consider all possible pairs, as depicted on the center in the figure below. Here the benefit is that the training set is smaller, though there is a larger number of classifiers. Though, it seems that you have to do more work, it is not the case, as the classification for each pair is much smaller, which can be easily paralellized.\n",
    "\n",
    "Another method, is the so called **error correcting output codes**, which is one of the most used methods nowadays, and it is an ensemble method. First of all a code matrix is created, which is a binary representation for each one of the classes, each code is different for each class and the binary representation is also called codeword. The next step is to use several binary classification algorithms on the same data, where each outcome is a binary value. These binary classification algorithms are called learners. Therefore, each new instance is evaluated through the learners and they are compared with the already assignated binary classification for the existing class (usually the so called Hamming distance is used for this process). If the distance is zero, the class exist and it can be assigned to that particular class. Usually, we use more binary classfications than those require to write the data in a binary representation, similar to the methods used in error correction in computer systems. The larger the number of learners, the performance of the classifier becomes better. A simple form of a binary representation is shown on the right hand side in the figure below.\n",
    "\n",
    "<table><tr><td><img src='fig/MultiClassClassification_1.jpeg' alt=\"OtoA\" style=\"width:90%\"></td><td><img src='fig/MultiClassClassification_2.jpeg' alt=\"OtoO\" style=\"width:90%\"></td><td><img src='fig/MultiClassClassification_3.jpeg' alt=\"ECOC\" style=\"width:90%\"></td></tr></table>\n",
    "\n",
    "\n",
    "Here we need to be aware about the quality type of data we will be using. For example, if we have imbalanced data, this typically refers to a problem with classification problems where the classes are not represented equally. Therefore before we use any algorithm, we should become familiar with the type of data we have at hand.\n",
    "\n",
    "In the following example we will use supporting vector machine as the classifier for our 10 dimensional data. As discussed before, SVC methods is a classifier where a hyperplane that separates the data in classes is formally defined. \n",
    "\n",
    "\n",
    "is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.\n",
    "Gamma is one of the regularization parameters and allows some nonlinear classification line. \n",
    "With low gamma, points far away from plausible separation plane are considered in calculation for the separation plane. Where as high gamma means the points close to plausible plane are considered in calculation.\n",
    "\n",
    "\n",
    "Here we are using the default kernel, as described in [Scikit SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). A simple understanding of what is kernel can be seen in **linear kernel**, where the equation used for prediction from a new input $x_i$set can be obtained from the dot product between the input and each support vector $a_i$ as calculated by $f(x) = B(0) + \\sum a_i x_i$, where the . SOme other kernels are polynomial, exponential, etc. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "svm_clf.fit(X_train[:1000], y_train[:1000]) # y_train, not y_train_5\n",
    "svm_clf.predict(X_train[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_5 = svm_clf.decision_function(X_train[:1])\n",
    "scores_for_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(scores_for_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.classes_[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr_clf = OneVsRestClassifier(SVC(gamma=\"auto\", random_state=42))\n",
    "ovr_clf.fit(X_train[:1000], y_train[:1000])\n",
    "ovr_clf.predict(X_train[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ovr_clf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.predict(X_train[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.decision_function(X_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, X_train_scaled[:1000], y_train[:1000], cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled[:1000], \n",
    "                                 y_train[:1000], cv=3)\n",
    "conf_mx = confusion_matrix(y_train[:1000], y_train_pred[:1000])\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conf_mx)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix):\n",
    "    \"\"\"If you prefer color and a colorbar\"\"\"\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(conf_mx, cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = sgd_clf.predict(X_train_scaled)\n",
    "\n",
    "cl_a, cl_b = 3, 5\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
    "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
    "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two classifications, larger than 7 or if it is even or odd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict(X_train[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the following cell may take a very long time (possibly hours depending on your hardware)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_knn_pred = knn_clf.predict(X_train[:100])\n",
    "f1_score(y_multilabel[:100], y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multioutput classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
    "X_train_mod = X_train + noise\n",
    "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
    "X_test_mod = X_test + noise\n",
    "y_train_mod = X_train\n",
    "y_test_mod = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 0\n",
    "plt.subplot(121); plot_digit(X_test_mod[some_index])\n",
    "plt.subplot(122); plot_digit(y_test_mod[some_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
    "plot_digit(clean_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = chapter_number\n",
    "maxt=(2*(n-1)+3)*np.pi/2\n",
    "t = np.linspace(np.pi/2, maxt, 1000)\n",
    "tt= 1.0/(t+0.01)\n",
    "x = (maxt-t)*np.cos(t)**3\n",
    "y = t*np.sqrt(np.abs(np.cos(t))) + np.sin(0.3*t)*np.cos(2*t)\n",
    "plt.plot(x, y, c=\"green\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f'Chapter {chapter_number} run in {int(end - start):d} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
