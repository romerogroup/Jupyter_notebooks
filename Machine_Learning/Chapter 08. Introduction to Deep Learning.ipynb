{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practical Machine Learning with Python\n",
    "# Chapter 8: From Neural Networks to Deep Learning\n",
    "## Guillermo Avendaño-Franco and Aldo Humberto Romero\n",
    "## West Virginia University\n",
    "\n",
    "### Machine Learning Workshop 2019\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on a variety of sources, usually other notebooks, the material was adapted to the topics covered during lessons. In some cases, the original notebooks were created for Python 2.x or older versions of Scikit-learn or Tensorflow and they have to be adapted. \n",
    "\n",
    "## References\n",
    "\n",
    "### Books\n",
    "\n",
    " * **Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems**, 1st Edition *Aurélien Géron*  (2017)\n",
    "\n",
    " * **Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow**, 2nd Edition, *Sebastian Raschka* and *Vahid Mirjalili* (2017)\n",
    "\n",
    " * **Deep Learning: A Practitioner's approach**, *Josh Patterson* and *Adam Gibson* \n",
    " \n",
    " * **Deep Learning**, *Ian Goodfelow*, *Yoshua Bengio* and *Aaron Courville* (2016)\n",
    "\n",
    "### Jupyter Notebooks\n",
    "\n",
    " * [Yale Digital Humanities Lab](https://github.com/YaleDHLab/lab-workshops)\n",
    " \n",
    " * Aurelein Geron Hands-on Machine Learning with Scikit-learn \n",
    "   [First Edition](https://github.com/ageron/handson-ml)\n",
    "   [Second Edition (In preparation)](https://github.com/ageron/handson-ml2)\n",
    "   \n",
    " * [A progressive collection notebooks of the Machine Learning course by the University of Turin](https://github.com/rugantio/MachineLearningCourse)\n",
    "   \n",
    " * [A curated set of jupyter notebooks about many topics](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)\n",
    "   \n",
    "### Videos\n",
    "\n",
    " * [Caltech's \"Learning from Data\" by Professor Yaser Abu-Mostafa](https://work.caltech.edu/telecourse.html)\n",
    " \n",
    " The support of the National Science Foundation and the US Department of Energy under projects: DMREF-NSF 1434897, NSF OAC-1740111 and DOE DE-SC0016176 is recognized.\n",
    "\n",
    "<div style=\"clear: both; display: table;\">\n",
    "<div style=\"border: none; float: left; width: 40%; padding: 10px\">\n",
    "<img src=\"fig/NSF.jpg\" alt=\"National Science Foundation\" style=\"width:50%\" align=\"left\">\n",
    "    </div>\n",
    "    <div style=\"border: none; float: right; width: 40%; padding: 10px\">\n",
    "<img src=\"fig/DOE.jpg\" alt=\"National Science Foundation\" style=\"width:50%\" align=\"right\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This Jupyter notebook was created to run on a Python 3 kernel. Some Ipython magics were used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands prefaced by a % in Jupyter are called \"magic\"\n",
    "# these \"magic\" commands allow us to do special things only related to jupyter\n",
    "\n",
    "# %matplotlib inline - allows one to display charts from the matplotlib library in a notebook\n",
    "# %load_ext autoreload - automatically reloads imported modules if they change\n",
    "# %autoreload 2 - automatically reloads imported modules if they change\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29T14:58:42-04:00\n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 5.8.0\n",
      "\n",
      "compiler   : GCC 8.3.0\n",
      "system     : Linux\n",
      "release    : 5.0.0-20-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib 3.0.2\n",
      "numpy      1.16.2\n",
      "tensorflow 1.14.0\n",
      "IPython    5.8.0\n",
      "scipy      1.2.1\n",
      "sklearn    0.20.2\n",
      "pandas     0.23.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material was elaborated from a variety of sources. Mostly from John Urbanic's \"Deep Learning In An Afternoon\".\n",
    "\n",
    "A very approachable introduction to Neural Networks with descriptive examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Neural Networks to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning is one of the techniques in Machine Learning with most success in a variety of problems. From Classification to Regression. Its ability to account for complexity is remarkable.\n",
    "\n",
    "An Artificial Neural Network (ANN) is a computational model that is inspired by the way biological neural networks in the human brain process information or it can be also thought a generalization of the Perceptron idea.\n",
    "\n",
    "To be specific:\n",
    "\n",
    "\n",
    "### Biological Neural Networks\n",
    "\n",
    "From one side the idea of simulate synapsis in biological Neural Networks and use the knowledge about activation barriers and multiple connectivity as inspiration to create and Artificial Neural Network. The basic computational unit of the brain is a neuron. Approximately 86 billion neurons can be found in the human nervous system and they are connected with approximately 10¹⁴ — 10¹⁵ synapses\n",
    "\n",
    "![Biological to Artificial Neural Networks](fig/bioNN.png)\n",
    "\n",
    "The idea with ANN is that synaptic strengths (the weights w in our mathematical model) are learnable and control the strength of influence and its direction: excitory (positive weight) or inhibitory (negative weight) of one neuron on another. If the final sum if the different connections is above a certain threshold, the neuron can fire, sending a spike along its axon, which is the output of the network under the provided input.\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "The other origin is the idea of Perceptron in Machine Learning. \n",
    "\n",
    "Perceptron is a linear classifier (binary), and used in supervised learning. It helps to classify the given input data. As a binary classifier it can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. \n",
    "\n",
    "<img src=\"fig/perceptron.png\" width=\"700\" height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity of Neural Networks\n",
    "\n",
    "![Complexity](fig/shallow_and_deep_NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the neural nework has no hidden layer it is in fact just a linear classifier or Perceptron. When hidden layers are added the NN is able to account for non-linearity, in the case of multiple hidden layers have what is called a Deep Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In practice how complex it can be\n",
    "\n",
    "A Deep Learning model can include:\n",
    "    \n",
    " * **Input** (With many neurons)\n",
    " * **Layer 1** \n",
    " * ...\n",
    " * ...\n",
    " * **Layer N**\n",
    " * **Output layer** (With many neurons)\n",
    "    \n",
    "For example the input can be an image with thousands of pixels and 3 colors for each pixel. Hundreds of hidden layers and the output could also be an image. That complexity is responsible for the computational cost of running such networks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Network Architecture\n",
    "\n",
    "![Basic Architecture](fig/basic_architeture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Zoo\n",
    "\n",
    "Since neural networks is one of the more active research fields in machine learning, a large number of modifications have been proposed. In the following figure, a summary of the different node sctructures is drawn and from that, relation and acronyms are provided such that the some of the different networks are related someway. The figure below shows a summary but let me give you a quick overview on few of them.\n",
    "\n",
    "1)  Feed forward neural networks (FF or FFNN) and perceptrons (P). They feed information from the front to the back (input and output, respectively). Neural networks are often described as having layers, where each layer consists of either input, hidden or output cells in parallel. A layer alone never has connections and in general two adjacent layers are fully connected (every neuron form one layer to every neuron to another layer). One usually trains FFNNs through back-propagation, giving the network paired datasets of “what goes in” and “what we want to have coming out”.  Given that the network has enough hidden neurons, it can theoretically always model the relationship between the input and output. Practically their use is a lot more limited but they are popularly combined with other networks to form new networks.\n",
    "\n",
    "2) Radial basis functions. This network is simpler than the normal FFNN, as the activation function is basically a radial function. Each RBFN neuron stores a “prototype”, which is just one of the examples from the training set. When we want to classify a new input, each neuron computes the Euclidean distance between the input and its prototype. Roughly speaking, if the input more closely resembles the class A prototypes than the class B prototypes, it is classified as class A.\n",
    "\n",
    "3) Recurrent Neural Networks (RNN). These netrowks are designed to take a series of input with no predetermined limit on size. Basically, they are networks with loops in them, allowing information to persist.\n",
    "\n",
    "4) Long / short term memory (LSTM) networks. There are a special kind of RNN where each neuron has a memory cell and three gates: input, output and forget. The idea of each gate is allow or stop the flow of information through them. \n",
    "\n",
    "5) Gated recurrent units (GRU) are a slight variation on LSTMs. They have one less gate and are wired slightly differently: instead of an input, output and a forget gate, they have an update gate.\n",
    "\n",
    "6) Convolutional Neural Networks (ConvNet) are very similar to FFNN, they are made up of neurons that have learnable weights and biases. In convolutional neural network (CNN, or ConvNet or shift invariant or space invariant) the unit connectivity pattern is inspired by the organization of the visual cortex, Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap, over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation. They are variations of multilayer perceptrons that use minimal preprocessing. Their wide applications is in image and video recognition, recommender systems and natural language processing. CNNs requires large data to train on.\n",
    "\n",
    "From <http://www.asimovinstitute.org/neural-network-zoo>\n",
    "    \n",
    "![Neural Network Zoo](fig/NeuralNetworkZoo20042019.png)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "Each internal neuron receives input from several other neurons, computes the aggregate and propagates the result based on the activation function. \n",
    "\n",
    "![Neural Network](fig/nn.png)\n",
    "\n",
    "\n",
    "![Activation Function](fig/activation_function.png)\n",
    "\n",
    "Neurons apply activation functions at these summed inputs. \n",
    "\n",
    "Activation functions are typically non-linear.\n",
    "\n",
    " * The **Sigmoid Function** produces a value between 0 and 1, so it is intuitive when a probability is desired, and was almost standard for many years.\n",
    " \n",
    " * The **Rectified Linear (ReLU)** activation function is zero when the input is negative and is equal to the input when the input is positive. Rectified Linear activation functions are currently the most popular activation function as they are more efficient than the sigmoid or hyperbolic tangent.\n",
    " \n",
    "     * Sparse activation: In a randomly initialized network, only 50% of hidden units are active.\n",
    "     \n",
    "     * Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions.\n",
    "     \n",
    "     * Efficient computation: Only comparison, addition and multiplication.\n",
    "     \n",
    "     * There are Leaky and Noisy variants.\n",
    "     \n",
    " * The **Soft Plus** shared some of the nice properties of ReLU and still preserves continuity on the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference or Foward Propagation\n",
    "\n",
    " <table>\n",
    "  <tr>\n",
    "    <th><img src=\"fig/DL1.png\" alt=\"Input\" style=\"width:400px\"></th>\n",
    "    <th><img src=\"fig/DL2.png\" alt=\"Hidden\" style=\"width:400px\"></th>\n",
    "    <th><img src=\"fig/DL3.png\" alt=\"Output\" style=\"width:400px\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Receiving Input</td>\n",
    "    <td>Computing Hidden Layer</td>\n",
    "    <td>Computing Output</td>\n",
    "  </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reciving Input\n",
    "\n",
    " * H1 Weights = (1.0, -2.0, 2.0)\n",
    " * H2 Weights = (2.0, 1.0, -4.0)\n",
    " * H3 Weights = (1.0, -1.0, 0.0)\n",
    " * O1 Weights = (-3.0, 1.0, -3.0)\n",
    " * O2 Weights = (0.0, 1.0, 2.0)\n",
    " \n",
    "### Hidden Layer\n",
    "\n",
    " * H1 = Sigmoid(0.5 * 1.0 + 0.9 * -2.0 + -0.3 * 2.0) = Sigmoid(-1.9) = .13\n",
    " * H2 = Sigmoid(0.5 * 2.0 + 0.9 * 1.0 + -0.3 * -4.0) = Sigmoid(3.1) = .96\n",
    " * H3 = Sigmoid(0.5 * 1.0 + 0.9 * -1.0 + -0.3 * 0.0) = Sigmoid(-0.4) = .40\n",
    " \n",
    "### Ouptut Layer\n",
    "\n",
    " * O1 = Sigmoid(.13 * -3.0 + .96 * 1.0 + .40 * -3.0) = Sigmoid(-.63) = .35\n",
    " * O2 = Sigmoid(.13 * 0.0 + .96 * 1.0 + .40 * 2.0) = Sigmoid(1.76) = .85\n",
    "\n",
    "\n",
    "In terms of Linear Algebra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -2.,  2.],\n",
       "       [ 2.,  1., -4.],\n",
       "       [ 1., -1.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden Layer Matrix\n",
    "H=np.array([[1,-2,2],[2,1,-4],[1,-1,0]],dtype=float)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0.9, -0.3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input vector\n",
    "inp=np.array([0.5,0.9,-0.3]).reshape(3)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9,  3.1, -0.4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Hidden layer operating over the input\n",
    "Hdotinp=np.dot(H,inp)\n",
    "Hdotinp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13010847, 0.95689275, 0.40131234])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "af=expit(Hdotinp)\n",
    "af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.,  1., -3.],\n",
       "       [ 0.,  1.,  2.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output matrix\n",
    "O=np.array([[-3.0, 1.0, -3.0],[0.0, 1.0, 2.0]])\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6373697 ,  1.75951742])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OdotAf=np.dot(O,af)\n",
    "OdotAf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34584136, 0.85314921])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expit(OdotAf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that we are able to describe the problem in terms of Linear Algebra is one of the reasons why Neural Networks are so efficient on GPUs. The same operation as a single execution line looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34584136, 0.85314921])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expit(np.dot(O,expit(np.dot(H,inp))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases\n",
    "\n",
    "It is also very useful to be able to offset our inputs by some constant. \n",
    "You can think of this as centering the activation function, or translating the solution (next slide). \n",
    "We will call this constant the bias, and it there will often be one value per layer.\n",
    "\n",
    "Our math for the previously calculated layer now looks like this with b=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32269997, 0.85959729])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expit(np.dot(O,expit(np.dot(H,inp) + np.full((3,), 0.1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for Non-Linearity\n",
    "\n",
    "Neural networks are so effective in classification and regression due to its ability to combine linear and non-linear operation on each step of the evaluation.\n",
    "\n",
    " * The matrix multiply provides the skew and scale.\n",
    " * The bias provides the translation.\n",
    " * The activation function provides the twist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hard part of Neural Networks: The back propagation\n",
    "\n",
    "During training, once we have forward propagated the network, we will find that the final output differs from the known output. The weights must need to be modified in order to produce better results in the next attempt. \n",
    "\n",
    "How do we know which new weights? to use? \n",
    "\n",
    "We want to minimize the error on our training data. \n",
    "Given labeled inputs, select weights that generate the smallest average error on the outputs.\n",
    "We know that the output is a function of the weights: \n",
    "\n",
    "\\begin{equation}\n",
    "E(w_1,w_2,w_3,...i_1,...t_1,...)\n",
    "\\end{equation}\n",
    "\n",
    "Just remember that the response of a single neuron can be writen as $f (b + \\sum_{i=1}^N a_i w_i$, where the $a_i$ is the output of the previous layer (or the input if it is the second layer) and $w_i$ are the weights. \n",
    "So to figure out which way, we need to change any particular weight, say $w_3$, we want to calculate\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E}{\\partial \\{w,i,t\\}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "If we use the chain rule repeatedly across layers we can work our way backwards from the output error through the weights, adjusting them as we go. Note that this is where the requirement that activation functions must have nicely behaved derivatives comes from.\n",
    "\n",
    "This technique makes the weight inter-dependencies much more tractable. \n",
    "An elegant perspective on this can be found from [Chris Olahat Blog](http://colah.github.io/posts/2015-08-Backprop)\n",
    "\n",
    "With basic calculus you can readily work through the details. \n",
    "\n",
    "You can find an excellent explanation from the renowned [3Blue1Brown](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the back propagation efficiently\n",
    "\n",
    "The explicit solution for back propagation leaves us with potentially many millions of simultaneous equations to solve (real nets have a lot of weights). \n",
    "\n",
    "They are non-linear to boot. Fortunately, this isn't a new problem created by deep learning, so we have options from the world of numerical methods.\n",
    "\n",
    "The standard has been **Gradient Descent** local minimization algorithms.\n",
    "\n",
    "\n",
    "To improve the convergence of Gradient Descent, refined methods use adaptive **time step** and incorporate **momentum** to help get over a local minimum. **Momentum** and **step size** are the two hyperparameter\n",
    "\n",
    "The optimization problem that Gradient Descent solves is a local minimization. We don't expect to ever find the actual global minimum. Several techniques have been created to avoid a solution being trapped in a local minima.\n",
    "\n",
    "We could/should find the error for all the training data before updating the weights (an epoch). However it is usually much more efficient to use a stochastic approach, sampling a random subset of the data, updating the weights, and then repeating with another. This is the **mini-batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One first example for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 12:00:22.471252 139805703444288 deprecation.py:323] From <ipython-input-11-ae79a13c5956>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0729 12:00:22.472553 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0729 12:00:22.473993 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0729 12:00:22.800023 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 12:00:23.070621 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0729 12:00:23.072955 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 12:00:23.333289 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0729 12:00:23.472496 139805703444288 deprecation.py:323] From <ipython-input-11-ae79a13c5956>:55: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 0 completed out of 5 loss: 1818695.238357544\n",
      "Epoch 1 completed out of 5 loss: 401376.5926427841\n",
      "Epoch 2 completed out of 5 loss: 221874.44502973557\n",
      "Epoch 3 completed out of 5 loss: 128293.66678369045\n",
      "Epoch 4 completed out of 5 loss: 79404.68307192624\n",
      "Accuracy: 0.9434\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "n_nodes_hl4 = 500\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    hidden_4_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_nodes_hl4])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl4]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl4, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    l4 = tf.add(tf.matmul(l2,hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
    "    l4 = tf.nn.relu(l4)\n",
    "\n",
    "    \n",
    "    output = tf.matmul(l4,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    # OLD VERSION:\n",
    "    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "    # NEW:\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 5\n",
    "    with tf.Session() as sess:\n",
    "        # OLD:\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        # NEW:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "A convolutional neural network (CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics.\n",
    "\n",
    "![CNN](fig/cnn.jpeg)\n",
    "\n",
    "\n",
    "As seen from this figure, CNN consists of a number of convolutional and subsampling layers optionally followed by fully connected layers. \n",
    "\n",
    "Let us say that our input to the convolutional layer is a $m \\times m \\times r$ pixels in an image where $m$ is the height and width of the image and $r$ is the number of channels, e.g. an RGB image has $r=3$. The convolutional layer will have $k$ filters (or kernels) of size $n \\times n \\times q$ where n is smaller than the dimension of the image and $q$ can either be the same as the number of channels r or smaller and may vary for each kernel. The size of the filters gives rise to the locally connected structure which are each convolved with the image to produce k feature maps of size $m−n+1$. \n",
    "\n",
    "A simple demonstration is shown in the figure below, where we assume a binary picture and a single filter of a 3x3 matrix. The primary purpose of Convolution is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. The orange square slide over the figure and for each 3x3 overlap, I multiply every element of the 3x3 submatrix of the figure with the convolution and then I add all elements afterwards. \n",
    "\n",
    "![ConvNet](fig/ConvNet.jpeg)\n",
    " \n",
    " \n",
    " It is clear that different values of the filter matrix will produce different Feature Maps for the same input image.\n",
    " \n",
    " Typical filter matrices are now described. \n",
    " \n",
    " For edge detection:\n",
    " $\n",
    "\\begin{bmatrix}\n",
    "1&0&-1\\\\\n",
    "0&0&0\\\\\n",
    "-1&0&1\\\\\n",
    "\\end{bmatrix}\n",
    "\\;\\;\n",
    "\\begin{bmatrix}\n",
    "0&1&0\\\\\n",
    "1&-4&1\\\\\n",
    "0&1&0\\\\\n",
    "\\end{bmatrix}\n",
    "\\;\\;\n",
    "\\begin{bmatrix}\n",
    "-1&-1&-1\\\\\n",
    "-1&8&-1\\\\\n",
    "-1&-1&-1\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "For sharpen:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "0&-1&0\\\\\n",
    "-1&5&-1\\\\\n",
    "0&-1&0\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "In practice, a CNN learns the values of these filters on its own during the training process (although we still need to specify parameters such as number of filters, filter size, architecture of the network etc. before the training process). The more number of filters we have, the more image features get extracted and the better our network becomes at recognizing patterns in unseen images.\n",
    "\n",
    "The other step that is described in this section is the pooling. Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first Neural Network using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1,shape=[32]))\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1,strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1,shape=[64]))\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2,strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1,shape=[1024]))\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1,shape=[10]))\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.16\n",
      "step 100, training accuracy 0.86\n",
      "step 200, training accuracy 0.82\n",
      "step 300, training accuracy 0.86\n",
      "step 400, training accuracy 0.94\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.92\n",
      "step 700, training accuracy 0.94\n",
      "step 800, training accuracy 0.94\n",
      "step 900, training accuracy 0.96\n",
      "test accuracy 0.9626\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I end and get more into Neural networks and different packages, I would like to discuss one of the most recent proposals in the literature. The so called Graph Neural Network. \n",
    "\n",
    "As many of the available information in fields like in  social network, knowledge graph, recommender system, and even life science comes in the form of graphs, very recently people have developed specific neural networks for these type of applications. Most of the discussion here has been taken from [Zhou's paper](https://arxiv.org/pdf/1812.08434.pdf).\n",
    "\n",
    "A Graph Neural Network is a type of Neural Network which directly operates on the Graph structure, which we define by a set of nodes and edges $G = (V, E)$. A typical application of GNN is node classification. Essentially, every node in the graph is associated with a label, and we want to predict the label of the nodes without ground-truth. Here we describe briefly this application and let the reader to search for more information.\n",
    "\n",
    "In the node classification problem setup, each node $V$ is characterized by its feature $x_v$ and associated with a ground-truth label $t_v$. Given a partially labeled graph $G$, the goal is to leverage these labeled nodes to predict the labels of the unlabeled. It learns to represent each node with a $d$ dimensional vector (state) $\\vec{h}_V$ which contains the information of its neighborhood. The state embedding $\\vec{h}_V$ is an $s$-dimension vector of node $V$ and can be used to produce an output $\\vec{o}_V$ such as the node label. Let $f$ be a parametric function, called local transition function, that is shared among all nodes and updates the node state according to the input neighborhood and let $g$ be the local output function that describes how the output is produced. Then, $\\vec{h}_V$ and $\\vec{o}_V$ are defined as follows:\n",
    "\n",
    "$$\n",
    "\\vec{h}_V = f ( \\vec{x}_V, \\vec{x}_{CO[V]}, \\vec{h}_{ne[V]}, \\vec{x}_{ne[V]})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{o}_V = g(\\vec{h}_V, \\vec{x}_V)\n",
    "$$\n",
    "\n",
    "where $\\vec{x}_V$, $\\vec{x}_{co[V]}$, $\\vec{h}_{ne[V]}$, $\\vec{x}_{ne[V]}$ are the features of $V$, the features of its edges, the states, and the features of the nodes in the neighborhood of $V$, respectively.\n",
    "\n",
    "Let $\\vec{H}$, $\\vec{O}$, $\\vec{X}$, and $\\vec{X}_N$ be the vectors constructed by stacking all the states, all the outputs, all the features, and all the node features, respectively. Then we have a compact form as:\n",
    "$$ \\vec{H} = F (\\vec{H}, \\vec{X}) $$\n",
    "$$ \\vec{O} = G(\\vec{H},\\vec{X}_N) $$\n",
    "where $F$, the global transition function, and $G$, the global output function are stacked versions of f and g for all nodes in a graph, respectively. The value of $\\vec{H}$ is the fixed point of Eq. 3 and is uniquely defined with the assumption that $F$ is a contraction map. Since we are seeking a unique solution for $\\vec{h}_v$, we can apply Banach fixed point theorem and rewrite the above equation as an iteratively update process. Such operation is often referred to as message passing or neighborhood aggregation.\n",
    "$$ \\vec{H}^{t+1} = F (\\vec{H}^t, \\vec{X}) $$\n",
    "where $\\vec{H}$ and $\\vec{X}$ denote the concatenation of all the $\\vec{h}$ and $\\vec{x}$, respectively.\n",
    "The output of the GNN is computed by passing the state h_v as well as the feature x_v to an output function g.\n",
    "$$ \\vec{o} = g(\\vec{h}_V,\\vec{x}_V) $$\n",
    "\n",
    "More details on this methodology can be found in that paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
