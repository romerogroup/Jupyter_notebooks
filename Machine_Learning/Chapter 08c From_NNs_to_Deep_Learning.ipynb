{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practical Machine Learning with Python\n",
    "# Chapter 8: From Neural Networks to Deep Learning\n",
    "## Guillermo Avendaño-Franco \n",
    "\n",
    "### HPC Summer Workshop 2019\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks is based on a variety of sources, usually other notebooks, the material was adapted to the topics covered during lessons. In some cases, the original notebooks were created for Python 2.x or older versions of Scikit-learn or Tensorflow and they have to be adapted. \n",
    "\n",
    "## References\n",
    "\n",
    "### Books\n",
    "\n",
    " * **Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems**, 1st Edition *Aurélien Géron*  (2017)\n",
    "\n",
    " * **Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow**, 2nd Edition, *Sebastian Raschka* and *Vahid Mirjalili* (2017)\n",
    "\n",
    " * **Deep Learning: A Practitioner's approach**, *Josh Patterson* and *Adam Gibson* \n",
    " \n",
    " * **Deep Learning**, *Ian Goodfelow*, *Yoshua Bengio* and *Aaron Courville* (2016)\n",
    "\n",
    "### Jupyter Notebooks\n",
    "\n",
    " * [Yale Digital Humanities Lab](https://github.com/YaleDHLab/lab-workshops)\n",
    " \n",
    " * Aurelein Geron Hands-on Machine Learning with Scikit-learn \n",
    "   [First Edition](https://github.com/ageron/handson-ml)\n",
    "   [Second Edition (In preparation)](https://github.com/ageron/handson-ml2)\n",
    "   \n",
    " * [A progressive collection notebooks of the Machine Learning course by the University of Turin](https://github.com/rugantio/MachineLearningCourse)\n",
    "   \n",
    " * [A curated set of jupyter notebooks about many topics](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)\n",
    "   \n",
    "### Videos\n",
    "\n",
    " * [Caltech's \"Learning from Data\" by Professor Yaser Abu-Mostafa](https://work.caltech.edu/telecourse.html)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This Jupyter notebook was created to run on a Python 3 kernel. Some Ipython magics were used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands prefaced by a % in Jupyter are called \"magic\"\n",
    "# these \"magic\" commands allow us to do special things only related to jupyter\n",
    "\n",
    "# %matplotlib inline - allows one to display charts from the matplotlib library in a notebook\n",
    "# %load_ext autoreload - automatically reloads imported modules if they change\n",
    "# %autoreload 2 - automatically reloads imported modules if they change\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29T14:58:42-04:00\n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 5.8.0\n",
      "\n",
      "compiler   : GCC 8.3.0\n",
      "system     : Linux\n",
      "release    : 5.0.0-20-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib 3.0.2\n",
      "numpy      1.16.2\n",
      "tensorflow 1.14.0\n",
      "IPython    5.8.0\n",
      "scipy      1.2.1\n",
      "sklearn    0.20.2\n",
      "pandas     0.23.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material was elaborated from a variety of sources. Mostly from John Urbanic's \"Deep Learning In An Afternoon\".\n",
    "\n",
    "A very approachable introduction to Neural Networks with descriptive examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Neural Networks to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning is one of the techniques in Machine Learning with most success in a variety of problems. From Classification to Regression. Its ability to account for complexity is remarkable.\n",
    "\n",
    "We can think about Neural Networks from two origins. \n",
    "\n",
    "### Biological Neural Networks\n",
    "\n",
    "From one side the idea of simulate synapsis in biological Neural Networks and use the knowledge about activation barriers and multiple connectivity as inspiration to create and Artificial Neural Network. \n",
    "\n",
    "![Biological to Artificial Neural Networks](fig/bioNN.png)\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "The other origin is the idea of Perceptron in Machine Learning.\n",
    "\n",
    "In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. As a binary classifier it can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. \n",
    "\n",
    "<img src=\"fig/perceptron.png\" width=\"700\" height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity of Neural Networks\n",
    "\n",
    "![Complexity](fig/shallow_and_deep_NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the neural nework has no hidden layer it is in fact just a linear classifier or Perceptron. When hidden layers are added the NN is able to account for non-linearity, in the case of multiple hidden layers have what is called a Deep Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In practice how complex it can be\n",
    "\n",
    "A Deep Learning model can include:\n",
    "    \n",
    " * **Input** (With many neurons)\n",
    " * **Layer 1** \n",
    " * ...\n",
    " * ...\n",
    " * **Layer N**\n",
    " * **Output layer** (With many neurons)\n",
    "    \n",
    "For example the input can be an image with thousands of pixels and 3 colors for each pixel. Hundreds of hidden layers and the output could also be an image. That complexity is responsible for the computational cost of running such networks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Network Architecture\n",
    "\n",
    "![Basic Architecture](fig/basic_architeture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Zoo\n",
    "\n",
    "From <http://www.asimovinstitute.org/neural-network-zoo>\n",
    "    \n",
    "![Neural Network Zoo](fig/NeuralNetworkZoo20042019.png)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "Each internal neuron receives input from several other neurons, computes the aggregate and propagates the result based on the activation function. \n",
    "\n",
    "![Neural Network](fig/nn.png)\n",
    "\n",
    "\n",
    "![Activation Function](fig/activation_function.png)\n",
    "\n",
    "Neurons apply activation functions at these summed inputs. \n",
    "\n",
    "Activation functions are typically non-linear.\n",
    "\n",
    " * The **Sigmoid Function** produces a value between 0 and 1, so it is intuitive when a probability is desired, and was almost standard for many years.\n",
    " \n",
    " * The **Rectified Linear (ReLU)** activation function is zero when the input is negative and is equal to the input when the input is positive. Rectified Linear activation functions are currently the most popular activation function as they are more efficient than the sigmoid or hyperbolic tangent.\n",
    " \n",
    "     * Sparse activation: In a randomly initialized network, only 50% of hidden units are active.\n",
    "     \n",
    "     * Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions.\n",
    "     \n",
    "     * Efficient computation: Only comparison, addition and multiplication.\n",
    "     \n",
    "     * There are Leaky and Noisy variants.\n",
    "     \n",
    " * The **Soft Plus** shared some of the nice properties of ReLU and still preserves continuity on the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference or Foward Propagation\n",
    "\n",
    " <table>\n",
    "  <tr>\n",
    "    <th><img src=\"fig/DL1.png\" alt=\"Input\" style=\"width:400px\"></th>\n",
    "    <th><img src=\"fig/DL2.png\" alt=\"Hidden\" style=\"width:400px\"></th>\n",
    "    <th><img src=\"fig/DL3.png\" alt=\"Output\" style=\"width:400px\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Receiving Input</td>\n",
    "    <td>Computing Hidden Layer</td>\n",
    "    <td>Computing Output</td>\n",
    "  </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reciving Input\n",
    "\n",
    " * H1 Weights = (1.0, -2.0, 2.0)\n",
    " * H2 Weights = (2.0, 1.0, -4.0)\n",
    " * H3 Weights = (1.0, -1.0, 0.0)\n",
    " * O1 Weights = (-3.0, 1.0, -3.0)\n",
    " * O2 Weights = (0.0, 1.0, 2.0)\n",
    " \n",
    "### Hidden Layer\n",
    "\n",
    " * H1 = Sigmoid(0.5 * 1.0 + 0.9 * -2.0 + -0.3 * 2.0) = Sigmoid(-1.9) = .13\n",
    " * H2 = Sigmoid(0.5 * 2.0 + 0.9 * 1.0 + -0.3 * -4.0) = Sigmoid(3.1) = .96\n",
    " * H3 = Sigmoid(0.5 * 1.0 + 0.9 * -1.0 + -0.3 * 0.0) = Sigmoid(-0.4) = .40\n",
    " \n",
    "### Ouptut Layer\n",
    "\n",
    " * O1 = Sigmoid(.13 * -3.0 + .96 * 1.0 + .40 * -3.0) = Sigmoid(-.63) = .35\n",
    " * O2 = Sigmoid(.13 * 0.0 + .96 * 1.0 + .40 * 2.0) = Sigmoid(1.76) = .85\n",
    "\n",
    "\n",
    "In terms of Linear Algebra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -2.,  2.],\n",
       "       [ 2.,  1., -4.],\n",
       "       [ 1., -1.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden Layer Matrix\n",
    "H=np.array([[1,-2,2],[2,1,-4],[1,-1,0]],dtype=float)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0.9, -0.3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input vector\n",
    "inp=np.array([0.5,0.9,-0.3]).reshape(3)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9,  3.1, -0.4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Hidden layer operating over the input\n",
    "Hdotinp=np.dot(H,inp)\n",
    "Hdotinp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13010847, 0.95689275, 0.40131234])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "af=expit(Hdotinp)\n",
    "af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.,  1., -3.],\n",
       "       [ 0.,  1.,  2.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output matrix\n",
    "O=np.array([[-3.0, 1.0, -3.0],[0.0, 1.0, 2.0]])\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6373697 ,  1.75951742])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OdotAf=np.dot(O,af)\n",
    "OdotAf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34584136, 0.85314921])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expit(OdotAf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that we are able to describe the problem in terms of Linear Algebra is one of the reasons why Neural Networks are so efficient on GPUs. The same operation as a single execution line looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34584136, 0.85314921])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expit(np.dot(O,expit(np.dot(H,inp))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases\n",
    "\n",
    "It is also very useful to be able to offset our inputs by some constant. \n",
    "You can think of this as centering the activation function, or translating the solution (next slide). \n",
    "We will call this constant the bias, and it there will often be one value per layer.\n",
    "\n",
    "Our math for the previously calculated layer now looks like this with b=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32269997, 0.85959729])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expit(np.dot(O,expit(np.dot(H,inp) + np.full((3,), 0.1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for Non-Linearity\n",
    "\n",
    "Neural networks are so effective in classification and regression due to its ability to combine linear and non-linear operation on each step of the evaluation.\n",
    "\n",
    " * The matrix multiply provides the skew and scale.\n",
    " * The bias provides the translation.\n",
    " * The activation function provides the twist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hard part of Neural Networks: The back propagation\n",
    "\n",
    "During training, once we have forward propagated the network, we will find that the final output differs from the known output. The weights must change in order to produce better results next time.\n",
    "\n",
    "How do we know which new weights? to use? \n",
    "\n",
    "We want to minimize the error on our training data. \n",
    "Given labeled inputs, select weights that generate the smallest average error on the outputs.\n",
    "We know that the output is a function of the weights: \n",
    "\n",
    "\\begin{equation}\n",
    "E(w_1,w_2,w_3,...i_1,...t_1,...)\n",
    "\\end{equation}\n",
    "\n",
    "So to figure out which way, and how much, to push any particular weight, say w3, we want to calculate\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E}{\\partial \\{w,i,t\\}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "If we use the chain rule repeatedly across layers we can work our way backwards from the output error through the weights, adjusting them as we go. Note that this is where the requirement that activation functions must have nicely behaved derivatives comes from.\n",
    "\n",
    "This technique makes the weight inter-dependencies much more tractable. \n",
    "An elegant perspective on this can be found from [Chris Olahat Blog](http://colah.github.io/posts/2015-08-Backprop)\n",
    "\n",
    "With basic calculus you can readily work through the details. \n",
    "\n",
    "You can find an excellent explanation from the renowned [3Blue1Brown](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the back propagation efficiently\n",
    "\n",
    "The explicit solution for back propagation leaves us with potentially many millions of simultaneous equations to solve (real nets have a lot of weights). \n",
    "\n",
    "They are non-linear to boot. Fortunately, this isn't a new problem created by deep learning, so we have options from the world of numerical methods.\n",
    "\n",
    "The standard has been **Gradient Descent** local minimization algorithms.\n",
    "\n",
    "\n",
    "To improve the convergence of Gradient Descent, refined methods use adaptive **time step** and incorporate **momentum** to help get over a local minimum. **Momentum** and **step size** are the two hyperparameter\n",
    "\n",
    "The optimization problem that Gradient Descent solves is a local minimization. We don't expect to ever find the actual global minimum. Several techniques have been created to avoid a solution being trapped in a local minima.\n",
    "\n",
    "We could/should find the error for all the training data before updating the weights (an epoch). However it is usually much more efficient to use a stochastic approach, sampling a random subset of the data, updating the weights, and then repeating with another. This is the **mini-batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One first example for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 12:00:22.471252 139805703444288 deprecation.py:323] From <ipython-input-11-ae79a13c5956>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0729 12:00:22.472553 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0729 12:00:22.473993 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0729 12:00:22.800023 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 12:00:23.070621 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0729 12:00:23.072955 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 12:00:23.333289 139805703444288 deprecation.py:323] From /home/gufranco/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0729 12:00:23.472496 139805703444288 deprecation.py:323] From <ipython-input-11-ae79a13c5956>:55: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 0 completed out of 5 loss: 1818695.238357544\n",
      "Epoch 1 completed out of 5 loss: 401376.5926427841\n",
      "Epoch 2 completed out of 5 loss: 221874.44502973557\n",
      "Epoch 3 completed out of 5 loss: 128293.66678369045\n",
      "Epoch 4 completed out of 5 loss: 79404.68307192624\n",
      "Accuracy: 0.9434\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "n_nodes_hl4 = 500\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    hidden_4_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_nodes_hl4])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl4]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl4, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    l4 = tf.add(tf.matmul(l2,hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
    "    l4 = tf.nn.relu(l4)\n",
    "\n",
    "    \n",
    "    output = tf.matmul(l4,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    # OLD VERSION:\n",
    "    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "    # NEW:\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 5\n",
    "    with tf.Session() as sess:\n",
    "        # OLD:\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        # NEW:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "![CNN](fig/cnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first Neural Network using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 12:00:46.085740 139805703444288 deprecation.py:506] From <ipython-input-13-b0b47f973ce1>:19: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1,shape=[32]))\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1,strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1,shape=[64]))\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2,strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1,shape=[1024]))\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1,shape=[10]))\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.16\n",
      "step 100, training accuracy 0.86\n",
      "step 200, training accuracy 0.82\n",
      "step 300, training accuracy 0.86\n",
      "step 400, training accuracy 0.94\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.92\n",
      "step 700, training accuracy 0.94\n",
      "step 800, training accuracy 0.94\n",
      "step 900, training accuracy 0.96\n",
      "test accuracy 0.9626\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
